[
  {
    "objectID": "homework/index.html",
    "href": "homework/index.html",
    "title": "Homework Solutions",
    "section": "",
    "text": "This section contains my solutions to the course homework assignments.\nHomework solutions will be added as assignments are completed.",
    "crumbs": [
      "Homework",
      "Homework Solutions"
    ]
  },
  {
    "objectID": "course-notes/index.html",
    "href": "course-notes/index.html",
    "title": "Contents",
    "section": "",
    "text": "This section contains notes from the pre-recorded lectures and slides.\n\n\n\nThe Garden of Forking Data",
    "crumbs": [
      "Course Notes",
      "Contents"
    ]
  },
  {
    "objectID": "course-notes/index.html#lectures",
    "href": "course-notes/index.html#lectures",
    "title": "Contents",
    "section": "",
    "text": "The Garden of Forking Data",
    "crumbs": [
      "Course Notes",
      "Contents"
    ]
  },
  {
    "objectID": "book-notes/index.html",
    "href": "book-notes/index.html",
    "title": "Contents",
    "section": "",
    "text": "This section contains my notes and exercises as I work through the Statistical Rethinking textbook.\n\n\n\nThe Golem of Prague\nSmall Worlds and Large Worlds",
    "crumbs": [
      "Book Notes",
      "Contents"
    ]
  },
  {
    "objectID": "book-notes/index.html#chapters",
    "href": "book-notes/index.html#chapters",
    "title": "Contents",
    "section": "",
    "text": "The Golem of Prague\nSmall Worlds and Large Worlds",
    "crumbs": [
      "Book Notes",
      "Contents"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Notes on Statistical Rethinking",
    "section": "",
    "text": "This website contains my notes and code as I work through Richard McElreath’s 2025 course on his book Statistical Rethinking. A course in Bayesian statistics, Rethinking is code-heavy and meant to be read sequentially, not as a reference. McElreath offers his flipped instruction course locally, also publishing the lectures online.\n\n\nThe repository is organized into separate quarto files, which are stitched together in _quarto.yml.\n\nbook-notes/: contains notes and exercises directly from the textbook\ncourse-notes/: contains notes from pre-recorded lectures and slides\nhomework/: contains homework solutions\n\n\n\n\nThe quarto notebooks use both R and Python in their code.\n\n\nFirst, install Quarto.\n\n\n\nInstall dependencies using uv:\nuv sync\nThis creates a virtual environment and installs packages from the lock file.\nNote: pymc compiles code at runtime, utilizing the g++ compiler it finds in the PATH.\n\n\n\nThis project uses R 4.5.2, which can be installed using rig:\nrig add 4.5.2\nThen, use rv to install the R dependencies:\nrv sync\n\n\n\nYou’ll need to have Hunspell installed for the spellcheck quarto extension to work.\nWindows:\nchoco install hunspell.portable\nMac:\nbrew install hunspell\nLinux:\nsudo apt-get install hunspell\n\n\nAfter installing Hunspell, you must install the en_US dictionary.\nWindows/Mac:\n\nDownload the dictionary files (both files: .dic and .aff).\nPlace them in hunspell’s dictionary SEARCH PATH.\n\n\nTo add a new directory to the search path, add the directory to the DICPATH environment variable.\nTo view the search path, use hunspell -D:\n\nLinux:\nsudo apt-get install hunspell-en-us\n\n\n\n\n\nAfter installing the necessary dependencies, build the project:\nquarto preview"
  },
  {
    "objectID": "index.html#organization",
    "href": "index.html#organization",
    "title": "Notes on Statistical Rethinking",
    "section": "",
    "text": "The repository is organized into separate quarto files, which are stitched together in _quarto.yml.\n\nbook-notes/: contains notes and exercises directly from the textbook\ncourse-notes/: contains notes from pre-recorded lectures and slides\nhomework/: contains homework solutions"
  },
  {
    "objectID": "index.html#dependencies",
    "href": "index.html#dependencies",
    "title": "Notes on Statistical Rethinking",
    "section": "",
    "text": "The quarto notebooks use both R and Python in their code.\n\n\nFirst, install Quarto.\n\n\n\nInstall dependencies using uv:\nuv sync\nThis creates a virtual environment and installs packages from the lock file.\nNote: pymc compiles code at runtime, utilizing the g++ compiler it finds in the PATH.\n\n\n\nThis project uses R 4.5.2, which can be installed using rig:\nrig add 4.5.2\nThen, use rv to install the R dependencies:\nrv sync\n\n\n\nYou’ll need to have Hunspell installed for the spellcheck quarto extension to work.\nWindows:\nchoco install hunspell.portable\nMac:\nbrew install hunspell\nLinux:\nsudo apt-get install hunspell\n\n\nAfter installing Hunspell, you must install the en_US dictionary.\nWindows/Mac:\n\nDownload the dictionary files (both files: .dic and .aff).\nPlace them in hunspell’s dictionary SEARCH PATH.\n\n\nTo add a new directory to the search path, add the directory to the DICPATH environment variable.\nTo view the search path, use hunspell -D:\n\nLinux:\nsudo apt-get install hunspell-en-us"
  },
  {
    "objectID": "index.html#build",
    "href": "index.html#build",
    "title": "Notes on Statistical Rethinking",
    "section": "",
    "text": "After installing the necessary dependencies, build the project:\nquarto preview"
  },
  {
    "objectID": "README.html",
    "href": "README.html",
    "title": "Notes on Statistical Rethinking",
    "section": "",
    "text": "This website contains my notes and code as I work through Richard McElreath’s 2025 course on his book Statistical Rethinking. A course in Bayesian statistics, Rethinking is code-heavy and meant to be read sequentially, not as a reference. McElreath offers his flipped instruction course locally, also publishing the lectures online.\n\n\nThe repository is organized into separate quarto files, which are stitched together in _quarto.yml.\n\nbook-notes/: contains notes and exercises directly from the textbook\ncourse-notes/: contains notes from pre-recorded lectures and slides\nhomework/: contains homework solutions\n\n\n\n\nThe quarto notebooks use both R and Python in their code.\n\n\nFirst, install Quarto.\n\n\n\nInstall dependencies using uv:\nuv sync\nThis creates a virtual environment and installs packages from the lock file.\nNote: pymc compiles code at runtime, utilizing the g++ compiler it finds in the PATH.\n\n\n\nThis project uses R 4.5.2, which can be installed using rig:\nrig add 4.5.2\nThen, use rv to install the R dependencies:\nrv sync\n\n\n\nYou’ll need to have Hunspell installed for the spellcheck quarto extension to work.\nWindows:\nchoco install hunspell.portable\nMac:\nbrew install hunspell\nLinux:\nsudo apt-get install hunspell\n\n\nAfter installing Hunspell, you must install the en_US dictionary.\nWindows/Mac:\n\nDownload the dictionary files (both files: .dic and .aff).\nPlace them in hunspell’s dictionary SEARCH PATH.\n\n\nTo add a new directory to the search path, add the directory to the DICPATH environment variable.\nTo view the search path, use hunspell -D:\n\nLinux:\nsudo apt-get install hunspell-en-us\n\n\n\n\n\nAfter installing the necessary dependencies, build the project:\nquarto preview"
  },
  {
    "objectID": "README.html#organization",
    "href": "README.html#organization",
    "title": "Notes on Statistical Rethinking",
    "section": "",
    "text": "The repository is organized into separate quarto files, which are stitched together in _quarto.yml.\n\nbook-notes/: contains notes and exercises directly from the textbook\ncourse-notes/: contains notes from pre-recorded lectures and slides\nhomework/: contains homework solutions"
  },
  {
    "objectID": "README.html#dependencies",
    "href": "README.html#dependencies",
    "title": "Notes on Statistical Rethinking",
    "section": "",
    "text": "The quarto notebooks use both R and Python in their code.\n\n\nFirst, install Quarto.\n\n\n\nInstall dependencies using uv:\nuv sync\nThis creates a virtual environment and installs packages from the lock file.\nNote: pymc compiles code at runtime, utilizing the g++ compiler it finds in the PATH.\n\n\n\nThis project uses R 4.5.2, which can be installed using rig:\nrig add 4.5.2\nThen, use rv to install the R dependencies:\nrv sync\n\n\n\nYou’ll need to have Hunspell installed for the spellcheck quarto extension to work.\nWindows:\nchoco install hunspell.portable\nMac:\nbrew install hunspell\nLinux:\nsudo apt-get install hunspell\n\n\nAfter installing Hunspell, you must install the en_US dictionary.\nWindows/Mac:\n\nDownload the dictionary files (both files: .dic and .aff).\nPlace them in hunspell’s dictionary SEARCH PATH.\n\n\nTo add a new directory to the search path, add the directory to the DICPATH environment variable.\nTo view the search path, use hunspell -D:\n\nLinux:\nsudo apt-get install hunspell-en-us"
  },
  {
    "objectID": "README.html#build",
    "href": "README.html#build",
    "title": "Notes on Statistical Rethinking",
    "section": "",
    "text": "After installing the necessary dependencies, build the project:\nquarto preview"
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "License",
    "section": "",
    "text": "Copyright (c) 2026 Josh Livingston\n\nWritten notes: CC0 1.0 Universal\nCode: MIT License\n\n\n\nUnofficial companion to Statistical Rethinking by Richard McElreath. My notes and code are meant to complement the book, not replace it. If you find these notes helpful, consider buying the book."
  },
  {
    "objectID": "LICENSE.html#acknowledgment",
    "href": "LICENSE.html#acknowledgment",
    "title": "License",
    "section": "",
    "text": "Unofficial companion to Statistical Rethinking by Richard McElreath. My notes and code are meant to complement the book, not replace it. If you find these notes helpful, consider buying the book."
  },
  {
    "objectID": "book-notes/chapter-02.html",
    "href": "book-notes/chapter-02.html",
    "title": "Chapter 2: Small Worlds and Large Worlds",
    "section": "",
    "text": "Statistical modeling exists dualistically, in a small world (contained by the model’s inherent logic) and a big world (the context in which the model will live).\nIn the small world, it is important to make sure the model is valid, consistent, and well-behaved.\nThe large world is the broader context in which the finalized model will exist. It is important to note that the statistical model always underrepresents the full context of the large world. Out of sample data emerge, and a model’s logical inconsistencies can surface.\nAdapting to the large world is a straightforward process, for nature. For Bayesian models, the process is much more complex. It is difficult to beat natural heuristics once a model identifies what to look for.\nChapter 2 focuses on the small world.",
    "crumbs": [
      "Book Notes",
      "Chapter 2: Small Worlds and Large Worlds"
    ]
  },
  {
    "objectID": "book-notes/chapter-02.html#the-garden-of-forking-data",
    "href": "book-notes/chapter-02.html#the-garden-of-forking-data",
    "title": "Chapter 2: Small Worlds and Large Worlds",
    "section": "2.1: The garden of forking data",
    "text": "2.1: The garden of forking data\nBayesian analysis lends itself to continually exploring alternatives. In this sense, Bayesian analysis creates a series of data “forks” in its quest to deduce.",
    "crumbs": [
      "Book Notes",
      "Chapter 2: Small Worlds and Large Worlds"
    ]
  },
  {
    "objectID": "book-notes/chapter-02.html#building-a-model",
    "href": "book-notes/chapter-02.html#building-a-model",
    "title": "Chapter 2: Small Worlds and Large Worlds",
    "section": "2.2: Building a model",
    "text": "2.2: Building a model\nDevelopment of a Bayesian model is best done cyclically:\n\nCreate a data story\nUpdate\nEvaluate\nRepeat\n\n\n2.2.1: A data story\nThe first step is to develop a story, built on either associations or causality, about the data, particularly its variation. Data stories involve descriptions of the large world, as well as the small world sampling process. This story is then put into formal terms of probability.\nRecall that any model may map back to multiple stories. Thus, model success does not necessarily imply confirmation of the story.\n\n\n2.2.2: Bayesian updating\nYou create a Bayesian model with an initial estimate, your prior. Then, as you sample data, your model learns from the new data, updating itself.\n\n\n2.2.3: Evaluate\nThe nature of Bayesian models guarantees that, within the small world, the model will achieve perfect inference. This means that extra care must be taken to evaluate the model within the large world, where small world success does not necessarily translate.\nRemember that a model’s assumptions are never 100% correct. Failure to disprove a model is therefore an error of science, not a sign of success.",
    "crumbs": [
      "Book Notes",
      "Chapter 2: Small Worlds and Large Worlds"
    ]
  },
  {
    "objectID": "book-notes/chapter-02.html#components-of-the-model",
    "href": "book-notes/chapter-02.html#components-of-the-model",
    "title": "Chapter 2: Small Worlds and Large Worlds",
    "section": "2.3: Components of the model",
    "text": "2.3: Components of the model\n\n2.3.1: Variables\nIn the context of Bayesian models, variables refer to either observable data or unobservable parameters.\n\n\n2.3.2: Definitions\nThe definitions of the relationships among variables constitute a model.\n\n2.3.2.1: Observed variables\nTo “count” an observed variable, we assign it a probability distribution, known as a likelihood.\n\n\n2.3.2.2: Unobserved variables\nTo “count” an unobserved variable (a parameter), assign it a prior, then update the prior as the model learns from the data.\nThe prior is part of the model; thus, it should be developed within the same cyclic development of the rest of the model.\nPriors are arbitrary, a choice made by the scientist. So, it is important to test many priors. This does not mean that the scientist has undue influence over the model. If the chosen prior is bad, the model will be bad.",
    "crumbs": [
      "Book Notes",
      "Chapter 2: Small Worlds and Large Worlds"
    ]
  },
  {
    "objectID": "book-notes/chapter-02.html#making-the-model-go",
    "href": "book-notes/chapter-02.html#making-the-model-go",
    "title": "Chapter 2: Small Worlds and Large Worlds",
    "section": "2.4: Making the model go",
    "text": "2.4: Making the model go\nOnce the components have been established, the Bayesian model will update the prior distributions until the posterior distribution is determined. The posterior contains the probability distribution of different parameters, conditioned on the data and model.\n\n2.4.1: Bayes’ theorem\nNote that the following equations are equivalent:\n\\[\n\\Pr(A,B) = \\Pr(A|B)\\Pr(B) \\tag{2.1}\n\\]\n\\[\n\\Pr(A,B) = \\Pr(B|A)\\Pr(A) \\tag{2.2}\n\\]\nBoth equations represent the joint probability space of \\(A\\) and \\(B\\). \\((2.1)\\) does so by first asking “what is the probability of \\(B\\)?” Then, let’s suppose \\(B\\) occurs. Now, \\(B\\) having occurred, “what is the probability of \\(A\\)?” Multiply the two results together. \\((2.2)\\) follows the same sequence but in reverse.\nThe result is logical equivalence:\n\\[\n\\Pr(A|B)\\Pr(B) = \\Pr(B|A)\\Pr(A) \\tag{2.3}\n\\]\nNote that this equivalence can also be reached mathematically, via the definition of conditional probability (in the discrete case) or conditional density (continuous).\nBayes’ theorem wonderfully exploits this equivalence:\n\\[\n\\Pr(B|A) = \\frac{\\Pr(A|B)\\Pr(B)}{\\Pr(A)} \\tag{2.4}\n\\]\nWhy is the Bayes’ theorem expression better? The answer lies in practice:\n\n\\(\\Pr(B|A)\\) is difficult to measure directly.\n\\(\\Pr(A|B)\\) is the modeled likelihood.\n\\(\\Pr(B)\\) is simply the prior.\n\\(\\Pr(A)\\) (see appendix) is not needed, due to the implied proportionality:\n\n\\[\n\\Pr(B|A) \\propto \\Pr(A|B)\\Pr(B) \\tag{2.5}\n\\]\n\\((2.5)\\) says that the posterior is proportional to the likelihood multiplied by the prior. Since we choose the prior, we simply have to calculate the likelihood to find the shape of the posterior. We don’t need the exact density values; any needed estimation can be performed on the proportional density.\nWhile Bayes’ theorem is used by any probability-based inference, Bayesian inference concerns specifically the generalization of Bayes’ theorem to abstractions like parameters and models.\n\n\n2.4.2: Motors\nRethinking introduces three motors that each approximate Bayesian inference:\n\nGrid approximation\nQuadratic approximation\nMarkov chain Monte Carlo (MCMC)\n\n\n\n2.4.3: Grid approximation\nSimilar to grid search in machine learning, grid approximation in Bayesian inference achieves great performance in approximating the posterior, using a discrete set of possible parameter values.\nGrid approximation follows 5 steps:\n\nDefine the parameter grid:\n\n\n\nShow code\nimport numpy as np\n\n# change this to increase/decrease approximation precision\nN_OUT = 20\np_grid = np.linspace(0, 1, N_OUT)\n\n\n\nCompute the likelihood at each parameter value:\n\n\n\nShow code\nimport scipy.stats as st\n\nlikelihood = st.binom.pmf(6, 9, p_grid)\n\n\n\nDefine the priors:\n\n\n\nShow code\npriors = {\n    \"Uniform\": np.ones(N_OUT),\n    \"Step Function\": (p_grid &gt;= 0.5).astype(float),\n    \"Exponential Peak\": np.exp(-5 * np.abs(p_grid - 0.5)),\n}\n\n\n\nCompute the posteriors by multiplying the priors and the likelihood:\n\n\n\nShow code\nposteriors = {\n    name: (likelihood * prior) / (likelihood * prior).sum()\n    for name, prior in priors.items()\n}\n\n\n\nAnalyze the results. Notice the effect of the prior choice:\n\n\n\nShow code\nimport matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(3, 1, figsize=(8, 15), sharey=True)\n\nfor ax, (name, posterior) in zip(axes, posteriors.items()):\n    ax.plot(p_grid, posterior, \"-o\")\n    ax.set_title(name)\n    ax.set_xlabel(\"Probability of success\")\n\naxes[0].set_ylabel(\"Posterior probability\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n2.4.4: Quadratic approximation\nGenerally, the area near the peak of the posterior will be nearly Gaussian. So, the posterior can be approximated by a Gaussian distribution.\nThis Gaussian approximation is known as quadratic approximation because the logarithm of a normal distribution is a parabola. Quadratic approximation is also referred to as:\n\nGaussian approximation\nLaplace approximation\n\nQuadratic approximation is convenient because you only ever need to estimate two parameters: mean and variance. Grid search, in contrast, scales exponentially with the number of parameters.\nQuadratic approximation is comprised of two steps:\n\nFind the maximum a posteriori (MAP).\nEstimate the distribution near the MAP.\n\n\nImplementation via Hessian\nSuppose we are interested in estimating parameter \\(\\theta\\) via quadratic approximation. That is,\n\\[\n\\theta \\sim \\mathcal{N}(\\mu, \\sigma^2). \\tag{2.6}\n\\]\nThe posterior of \\(\\theta\\) is thus provided by the pmf. \\[\np(\\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\n            \\exp\\left(-\\frac{(\\theta - \\mu)^2}{2\\sigma^2}\\right).\n\\tag{2.7}\n\\]\nTaking the natural log provides the log-posterior.\n\\[\nf(\\theta) = \\ln(p(\\theta)) = -\\frac{1}{2}\\ln(2\\pi\\sigma^2)-\n                             \\frac{(\\theta - \\mu)^2}{2\\sigma^2}.\n\\tag{2.8}\n\\]\nWe are interested in the shape of the distribution near the mode. We label this peak \\(\\hat{\\theta}\\).\nTo approximate \\(f(\\theta)\\), we use a Taylor expansion.\n\\[\nf(\\theta) \\approx f(\\hat{\\theta}) +\n                  f'(\\hat{\\theta})(\\theta - \\hat{\\theta}) +\n                  \\frac{1}{2}f''(\\hat{\\theta})(\\theta - \\hat{\\theta})^2.\n\\tag{2.9}\n\\]\nSince \\(\\hat{\\theta}\\) is the peak of the distribution, we know that \\(f'(\\hat{\\theta})\\) equals 0. This simplifies \\(f(\\theta)\\).\n\\[\nf(\\theta) \\approx \\text{const} +\n                  \\frac{1}{2}f''(\\hat{\\theta})(\\theta - \\hat{\\theta})^2.\n\\tag{2.10}\n\\]\nSince \\(f(\\theta)\\) equals \\(\\ln(p(\\theta))\\), we set the equations approximately equal to each other.\n\\[\n-\\frac{1}{2}\\ln(2\\pi\\sigma^2)- \\frac{(\\theta - \\mu)^2}{2\\sigma^2}\n\\approx\n\\text{const} + \\frac{1}{2}f''(\\hat{\\theta})(\\theta - \\hat{\\theta})^2.\n\\tag{2.11}\n\\]\nIgnoring the constant terms shows us that\n\\[\n-\\frac{1}{2\\sigma^2}(\\theta - \\mu)^2\n\\approx\n\\frac{1}{2}f''(\\hat{\\theta})(\\theta - \\hat{\\theta})^2.\n\\tag{2.12}\n\\]\nThus,\n\\[\n-\\frac{1}{\\sigma^2} \\approx f''(\\hat{\\theta}).\n\\tag{2.13}\n\\]\nThe Hessian, \\(H\\), is the matrix of partial second derivatives. In other terms,\n\\[\nf''(\\hat{\\theta}) = H.\n\\tag{2.14}\n\\]\nSo, we can rewrite \\(\\sigma\\) in terms of the Hessian.\n\\[\n\\begin{align}\n-\\frac{1}{\\sigma^2} &\\approx H \\tag{2.15} \\\\\n\\sigma^2 &\\approx -\\frac{1}{H} \\tag{2.16} \\\\\n\\sigma &\\approx \\sqrt{-\\frac{1}{H}}. \\tag{2.17}\n\\end{align}\n\\]\nThe pymc translation provides an implementation of quadratic approximation that utilizes these special properties of the normal distribution. My class below organizes the fitting code together with the analysis and plotting.\n\n\nShow code\nimport pymc as pm\n\n\nclass BinomialQuadraticApproximation:\n    def __init__(self, W: int, L: int):\n        self.W = W\n        self.L = L\n        self._mu_last = None\n        self._sigma_last = None\n\n    @property\n    def n(self) -&gt; int:\n        \"\"\"\n        Number of trials. Equals W + L.\n        \"\"\"\n        return self.W + self.L\n\n    @property\n    def last_fit(self):\n        \"\"\"\n        Returns mu, sigma (None, None if .fit() has not been called)\n        \"\"\"\n        return self._mu_last, self._sigma_last\n\n    def ci(self, q):\n        \"\"\"\n        Given quantile, calcualtes credible interval for most recent fit\n        \"\"\"\n        mu, sigma = self.last_fit\n        if mu is None or sigma is None:\n            raise ValueError(\"Run .fit() before computing credible intervals\")\n        return st.norm.ppf(q, loc=mu, scale=sigma)\n\n    def plot(self, ax=None):\n        mu, sigma = self.last_fit\n        if mu is None or sigma is None:\n            raise ValueError(\"Run .fit() before plotting\")\n\n        a = self.W + 1\n        b = self.L + 1\n        x = np.linspace(0, 1)\n        if ax is None:\n            fig, ax = plt.subplots(1, 1)\n\n        ax.plot(x, st.beta.pdf(x, a, b), \"r-\", label=\"Analytical Posterior\")\n        ax.plot(x, st.norm.pdf(x, mu, sigma), \"b-\", label=\"Quadratic Approximation\")\n\n        ax.set_title(f\"n = {self.n}\")\n        ax.legend()\n\n    def fit(self):\n        \"\"\"\n        Objective is to identify mu and sigma to fit the posterior normal\n        distribution.\n\n        mu is a model parameter, the average probability of success. mu receives\n        a uniform prior and its MAP will be used for inference\n\n        sigma is obtained using the hessian at the MAP.\n\n        Taken together, N(mu, sigma) represents the posterior distribution of the\n        probability of success.\n\n        # Notes:\n\n        ## `default_transform=None`\n        - In pymc5, `default_transform=None` can be added to the parameter,\n          rather than as tags after the fact.\n          - Transformation to log-odds are performed by default on bounded\n            parameters\n          - We disable so that we may compute the hessian on the untransformed\n            space\n\n        ## 'Powell' optimizer\n        - Contrary to the translation example, here, the default optimizer does\n            not move `p` when `default_transform=None`\n        - 'Powell' returns 0.67 as expected\n        \"\"\"\n        with pm.Model() as model:\n            # parameters\n            ## p: prior: uniform over [0,1]\n            p = pm.Uniform(\"p\", 0, 1, default_transform=None)\n\n            # likelihood: binomial\n            x = pm.Binomial(\"x\", n=self.n, p=p, observed=self.W)\n\n            # find MAP\n            MAP = pm.find_MAP(method=\"Powell\")\n\n            # estimate sigma at MAP\n            hessian = pm.find_hessian(MAP, vars=[p], negate_output=False)\n            sigma = np.sqrt(-np.linalg.inv(hessian))[0][0]\n\n        self._set_last_fit(MAP[\"p\"], sigma)\n        return self.last_fit\n\n    def _set_last_fit(self, mu, sigma):\n        self._mu_last = mu\n        self._sigma_last = sigma\n\n\n/home/runner/work/statistical-rethinking-notes/statistical-rethinking-notes/.venv/lib/python3.12/site-packages/arviz/__init__.py:39: FutureWarning: \nArviZ is undergoing a major refactor to improve flexibility and extensibility while maintaining a user-friendly interface.\nSome upcoming changes may be backward incompatible.\nFor details and migration guidance, visit: https://python.arviz.org/en/latest/user_guide/migration_guide.html\n  warn(\n\n\nWe can use the final parameter values to analyze our distribution:\n\n\nShow code\nmodel = BinomialQuadraticApproximation(6, 3)\nmu, sigma = model.fit()\nci_lower = model.ci(0.055)\nci_upper = model.ci(0.945)\n\n\n/home/runner/work/statistical-rethinking-notes/statistical-rethinking-notes/.venv/lib/python3.12/site-packages/rich\n/live.py:260: UserWarning: install \"ipywidgets\" for Jupyter support\n  warnings.warn('install \"ipywidgets\" for Jupyter support')\n\n\n\n/home/runner/work/statistical-rethinking-notes/statistical-rethinking-notes/.venv/lib/python3.12/site-packages/scip\ny/optimize/_optimize.py:2540: RuntimeWarning: invalid value encountered in scalar multiply\n  tmp2 = (x - v) * (fx - fw)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nMean (MAP)\nStd Dev\n5.5% Credible Interval (CI)\n94.5% CI\n\n\n\n\n\\(p\\)\n0.67\n0.16\n0.42\n0.92\n\n\n\nWe can compare the “true” posterior with the quadratic approximation:\n\n\nShow code\nw_values = [6, 12, 24]\nfig, axes = plt.subplots(1, len(w_values), figsize=(5*len(w_values),5))\nfor i, w in enumerate(w_values):\n    model = BinomialQuadraticApproximation(w, int(w * 0.5))\n    model.fit()\n    model.plot(ax=axes[i])\nplt.show()\n\n\n/home/runner/work/statistical-rethinking-notes/statistical-rethinking-notes/.venv/lib/python3.12/site-packages/rich\n/live.py:260: UserWarning: install \"ipywidgets\" for Jupyter support\n  warnings.warn('install \"ipywidgets\" for Jupyter support')\n\n\n\n/home/runner/work/statistical-rethinking-notes/statistical-rethinking-notes/.venv/lib/python3.12/site-packages/scip\ny/optimize/_optimize.py:2540: RuntimeWarning: invalid value encountered in scalar multiply\n  tmp2 = (x - v) * (fx - fw)\n\n\n\n\n\n\n/home/runner/work/statistical-rethinking-notes/statistical-rethinking-notes/.venv/lib/python3.12/site-packages/rich\n/live.py:260: UserWarning: install \"ipywidgets\" for Jupyter support\n  warnings.warn('install \"ipywidgets\" for Jupyter support')\n\n\n\n/home/runner/work/statistical-rethinking-notes/statistical-rethinking-notes/.venv/lib/python3.12/site-packages/scip\ny/optimize/_optimize.py:2540: RuntimeWarning: invalid value encountered in scalar multiply\n  tmp2 = (x - v) * (fx - fw)\n\n\n\n\n\n\n/home/runner/work/statistical-rethinking-notes/statistical-rethinking-notes/.venv/lib/python3.12/site-packages/rich\n/live.py:260: UserWarning: install \"ipywidgets\" for Jupyter support\n  warnings.warn('install \"ipywidgets\" for Jupyter support')\n\n\n\n/home/runner/work/statistical-rethinking-notes/statistical-rethinking-notes/.venv/lib/python3.12/site-packages/scip\ny/optimize/_optimize.py:2540: RuntimeWarning: invalid value encountered in scalar multiply\n  tmp2 = (x - v) * (fx - fw)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.4.5: Markov chain Monte Carlo\nSince grid approximation scales poorly and quadratic approximation does not fit most scenarios, Markov chain Monte Carlo (MCMC), along with other model fitting techniques, have been developed.\nMCMC works by sampling from the posterior directly; the result is a sampling distribution of the parameters.\nThis pymc5 implementation uses the Metropolis Algorithm, explained in Chapter 9.\n\n\nShow code\nW = 6\nL = 3\nN = 1000\np = np.repeat(0.0, N)\np[0] = 0.5\nq0 = st.binom.pmf(W, W + L, p[0])\nfor i in range(1, N):\n    # Propose a value\n    p_new = st.norm.rvs(p[i - 1], 0.1)\n\n    # Enforce [0, 1]\n    while p_new &lt; 0:\n        p_new = np.abs(p_new)\n        while p_new &gt; 1:\n            p_new = 2 - p_new\n\n    # Accept/Reject\n    q1 = st.binom.pmf(W, W + L, p_new)\n    if st.uniform.rvs(0, 1) &lt; q1 / q0:\n        # Accept\n        p[i] = p_new\n        q0 = q1\n    else:\n        # Reject\n        p[i] = p[i - 1]\n\n\nThe resulting sampling distribution can be visualized alongside its estimated density and the analytical posterior.\n\n\nShow code\nimport arviz as az\n\nx = np.linspace(0, 1)\nfig, ax = plt.subplots(1, 1)\nax2 = ax.twinx()\n\nax2.plot(x, st.beta.pdf(x, W + 1, L + 1), \"r-\", label=\"Analytical posterior\")\n\naz.plot_kde(p, label=\"Metropolis approximation\")\nax.hist(p, bins=30)\n\nfig.tight_layout()\nax.legend()\nplt.show()\n\n\n/tmp/ipykernel_5901/294223342.py:13: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n  ax.legend()",
    "crumbs": [
      "Book Notes",
      "Chapter 2: Small Worlds and Large Worlds"
    ]
  },
  {
    "objectID": "book-notes/chapter-02.html#appendix",
    "href": "book-notes/chapter-02.html#appendix",
    "title": "Chapter 2: Small Worlds and Large Worlds",
    "section": "Appendix",
    "text": "Appendix\n\nA2.4.1: Bayes’ theorem\nNote that \\(\\Pr(A)\\) is measurable via the Law of Total Probability:\n\\[\n\\Pr(A) = \\mathbb{E}[\\Pr(A|B)] = \\int{\\Pr(A|B)\\Pr(B)dB} \\tag{A2.1}\n\\]\n\\(\\mathbb{E}[\\Pr(A|B)]\\) is also referred to as the marginal likelihood.",
    "crumbs": [
      "Book Notes",
      "Chapter 2: Small Worlds and Large Worlds"
    ]
  },
  {
    "objectID": "book-notes/chapter-01.html",
    "href": "book-notes/chapter-01.html",
    "title": "Chapter 1: The Golem of Prague",
    "section": "",
    "text": "Scientists frequently construct and use “statistical golems” - tools that know their own procedure, but have no inherent wisdom.\n\np-values, stat tests, etc\n\nClassical statistical tools may not be adaptable to all modern research scenarios.\nThe golems are often associative rather than causal, leading to misuse.\n\n\n\n\n\nThe classic approach is to use a “flow-chart” to determine the statistical procedure to use. However, this can lead to battles over picking the “correct” test.\nWe also face epistemological issues, realized as conflation between causation and hypothesis falsification.\n\nKarl Popper’s The Myth of the Framework (1996), his last book, covers this topic.\n\nMcElreath posits that “deductive falsification is impossible”, given that:\n\nHypotheses are not models. So, falsifying the hypothesis does not falsify the model.\nMeasurement can invalidate models. If the measurement is wrong, falsification is not possible.\nNote that null hypothesis significance testing (NHST) attempts to falsify the null hypothesis, not the actual research hypothesis.\n\n\n\n\nHypotheses are not models; rather they are one of three pieces that constitute scientific understanding:\n\nHypotheses are falsifiable, often vague descriptions of evidence.\nProcess models are non-statistical understandings of a mechanism which supports or refutes a hypothesis.\nStatistical models are statistical understandings of process models.\n\nThese pieces have special properties: - Since hypotheses are vague, they map to multiple process models. - Effectively similarly, statistical models can map to multiple process models. - Therefore, any statistical model can support/refute multiple hypotheses.\nThis conundrum implies that if two models imply similar data, you should search for a description of the data under which the processes look different. Two process models can make similar predictions for field X, but vastly different predictions for Y.\n\n\n\nTwo properties of statistical modeling complicate hypothesis falsification:\n\nObservation error\nContinuous hypotheses\n\n\n\nDifficulty of observation often leads to observed data mapping to multiple hypothetical conclusions. Or, the data can simply be measured incorrectly. Both instances can result in “spurious falsifications,” where the conclusions drawn are due to chance.\n\n\n\nContinuous hypotheses, such as “80% of swans are white” are difficult to prove, since Modus tollens (“If P, then Q. Not Q. So, not P.”) does not apply.\n\n\n\n\nIn the scientific community, falsification of hypotheses is not strictly logical; rather, agreements on evidence are reached via consensus. Consequently, claims of science’s definitiveness are usually exaggerated and possibly societally harmful. Kitcher (2011), Science in a Democratic Society, is a good intro to the sociology of science.\n\n\n\n\nScientific research is often broader in scope than testing alone; models fill this gap. In addition to serving testing, models can make predictions and communicate understandings.\nRethinking covers four tools for modeling:\n\nBayesian data analysis\nModel comparison\nMultilevel models\nGraphical causal models\n\n\n\n\nWe just use randomness to describe our uncertainty…\n\nBayesian data analysis is a highly generalizable probability-based tool for using data to learn about the world.\nBayesian analysis is intuitive:\n\nCount* the number of possible explanations.\nIdentify the most plausible explanations based on counts.\n\n*In Bayesian, “count” refers to calculus, since Bayesian is a practice of probability distributions.\nThe frequentist approach, in contrast, imagines an infinite resampling of data to reach a probability distribution. Ironically, however, frequentist tests are often interpreted as Bayesian components (“95% likely that the value falls inside this confidence interval”, e.g).\nFurther, in the frequentist approach, parameters are not random variables. Random variables are useful when the measurement leads to a uniform sampling distribution. In the case of image analysis, Bayesian analysis is often used, allowing noisy images to be reconstructed using modeled probabilities.\n\n\n\n\nFitting is easy; prediction is hard.\n\nCross-validation and information criteria are two metrics used to estimate predictive accuracy and evaluate possibility of overfitting:\n\nCross-validation provides provide fit estimates over different resamples of the data.\nInformation criteria are measures that “balance model fit with model complexity”.\n\nRecall that multiple process models can exist for a statistical model. Thus, it is always necessary to compare multiple statistical models for a given problem.\n\n\n\nMultilevel models are hierarchical Bayesian models that utilize partial pooling, a statistical technique that uses information across levels of the hierarchy to produce better estimates for units within a level.\nFour common applications of partial pooling:\n\nAdjust for repeat sampling (multiple observations from the same unit)\nAdjust for imbalanced sampling (observation X sampled 10x more than Y)\nStudy variation (useful when question includes variations among a level within the hierarchy)\nAvoid averaging (features are often pre-averaged for traditional models, destroying the data’s variation)\n\nMcElreath argues that “multilevel regression deserves to be the default form of regression,” and that research involving single-level models must justify the decision to not use a multilevel model. Particularly, the researcher must demonstrate lack of variation in treatment effects among observations.\nMultilevel models are not new! From 1960 to 1978 John Tukey developed multilevel models for NBC from that were used for election forecasting.\n\n\n\n\nA statistical model is an amazing association engine\n\nFlawed causal models and confounding variables can often lead to better prediction, due to association’s predictive power. This often means that scientists can be systematically misled by predictive accuracy.\nThus, being mindful of the underlying causal assumptions is a mandatory component of statistical modeling. This is most often achieved through construction of a causal model, from which multiple statistical models can be developed.\nGraphical causal models, such as directed acyclic graphs (DAGs), are helpful tools allowing for the visualization of causal relationships. DAGs are developed outside of the data, usually in deep consultation with domain experts.\nToday’s dominating method of causal inference is the causal salad, wherein control variables are carefully tossed into the models, toward the aim of developing a causal narrative. When designing a model to be later used for interventions, it is important that the causal relationships, especially the controlling effects, are explainable.\n\n\n\n\nMcElreath makes the argument that instead of working with a plethora of black-box models, we should aim to develop the skills to analyze null hypotheses. The tools introduced – Bayesian data analysis, model comparison, multilevel modeling, and graphical causal models – serve this aim.\n\n\n\nChapters 2-3: Intro to Bayesian inference\nChapters 4-8: Multiple linear regression\nChapters 9-12: Generalized linear models\nChapters 13-16: Multilevel models\nChapter 17: Address issues from 1st edition\n\nEach chapter ends with exercises across difficulty levels. The more difficult problems introduce new material and challenges.",
    "crumbs": [
      "Book Notes",
      "Chapter 1: The Golem of Prague"
    ]
  },
  {
    "objectID": "book-notes/chapter-01.html#statistical-golems",
    "href": "book-notes/chapter-01.html#statistical-golems",
    "title": "Chapter 1: The Golem of Prague",
    "section": "",
    "text": "Scientists frequently construct and use “statistical golems” - tools that know their own procedure, but have no inherent wisdom.\n\np-values, stat tests, etc\n\nClassical statistical tools may not be adaptable to all modern research scenarios.\nThe golems are often associative rather than causal, leading to misuse.",
    "crumbs": [
      "Book Notes",
      "Chapter 1: The Golem of Prague"
    ]
  },
  {
    "objectID": "book-notes/chapter-01.html#statistical-rethinking",
    "href": "book-notes/chapter-01.html#statistical-rethinking",
    "title": "Chapter 1: The Golem of Prague",
    "section": "",
    "text": "The classic approach is to use a “flow-chart” to determine the statistical procedure to use. However, this can lead to battles over picking the “correct” test.\nWe also face epistemological issues, realized as conflation between causation and hypothesis falsification.\n\nKarl Popper’s The Myth of the Framework (1996), his last book, covers this topic.\n\nMcElreath posits that “deductive falsification is impossible”, given that:\n\nHypotheses are not models. So, falsifying the hypothesis does not falsify the model.\nMeasurement can invalidate models. If the measurement is wrong, falsification is not possible.\nNote that null hypothesis significance testing (NHST) attempts to falsify the null hypothesis, not the actual research hypothesis.\n\n\n\n\nHypotheses are not models; rather they are one of three pieces that constitute scientific understanding:\n\nHypotheses are falsifiable, often vague descriptions of evidence.\nProcess models are non-statistical understandings of a mechanism which supports or refutes a hypothesis.\nStatistical models are statistical understandings of process models.\n\nThese pieces have special properties: - Since hypotheses are vague, they map to multiple process models. - Effectively similarly, statistical models can map to multiple process models. - Therefore, any statistical model can support/refute multiple hypotheses.\nThis conundrum implies that if two models imply similar data, you should search for a description of the data under which the processes look different. Two process models can make similar predictions for field X, but vastly different predictions for Y.\n\n\n\nTwo properties of statistical modeling complicate hypothesis falsification:\n\nObservation error\nContinuous hypotheses\n\n\n\nDifficulty of observation often leads to observed data mapping to multiple hypothetical conclusions. Or, the data can simply be measured incorrectly. Both instances can result in “spurious falsifications,” where the conclusions drawn are due to chance.\n\n\n\nContinuous hypotheses, such as “80% of swans are white” are difficult to prove, since Modus tollens (“If P, then Q. Not Q. So, not P.”) does not apply.\n\n\n\n\nIn the scientific community, falsification of hypotheses is not strictly logical; rather, agreements on evidence are reached via consensus. Consequently, claims of science’s definitiveness are usually exaggerated and possibly societally harmful. Kitcher (2011), Science in a Democratic Society, is a good intro to the sociology of science.",
    "crumbs": [
      "Book Notes",
      "Chapter 1: The Golem of Prague"
    ]
  },
  {
    "objectID": "book-notes/chapter-01.html#tools-for-golem-engineering",
    "href": "book-notes/chapter-01.html#tools-for-golem-engineering",
    "title": "Chapter 1: The Golem of Prague",
    "section": "",
    "text": "Scientific research is often broader in scope than testing alone; models fill this gap. In addition to serving testing, models can make predictions and communicate understandings.\nRethinking covers four tools for modeling:\n\nBayesian data analysis\nModel comparison\nMultilevel models\nGraphical causal models\n\n\n\n\nWe just use randomness to describe our uncertainty…\n\nBayesian data analysis is a highly generalizable probability-based tool for using data to learn about the world.\nBayesian analysis is intuitive:\n\nCount* the number of possible explanations.\nIdentify the most plausible explanations based on counts.\n\n*In Bayesian, “count” refers to calculus, since Bayesian is a practice of probability distributions.\nThe frequentist approach, in contrast, imagines an infinite resampling of data to reach a probability distribution. Ironically, however, frequentist tests are often interpreted as Bayesian components (“95% likely that the value falls inside this confidence interval”, e.g).\nFurther, in the frequentist approach, parameters are not random variables. Random variables are useful when the measurement leads to a uniform sampling distribution. In the case of image analysis, Bayesian analysis is often used, allowing noisy images to be reconstructed using modeled probabilities.\n\n\n\n\nFitting is easy; prediction is hard.\n\nCross-validation and information criteria are two metrics used to estimate predictive accuracy and evaluate possibility of overfitting:\n\nCross-validation provides provide fit estimates over different resamples of the data.\nInformation criteria are measures that “balance model fit with model complexity”.\n\nRecall that multiple process models can exist for a statistical model. Thus, it is always necessary to compare multiple statistical models for a given problem.\n\n\n\nMultilevel models are hierarchical Bayesian models that utilize partial pooling, a statistical technique that uses information across levels of the hierarchy to produce better estimates for units within a level.\nFour common applications of partial pooling:\n\nAdjust for repeat sampling (multiple observations from the same unit)\nAdjust for imbalanced sampling (observation X sampled 10x more than Y)\nStudy variation (useful when question includes variations among a level within the hierarchy)\nAvoid averaging (features are often pre-averaged for traditional models, destroying the data’s variation)\n\nMcElreath argues that “multilevel regression deserves to be the default form of regression,” and that research involving single-level models must justify the decision to not use a multilevel model. Particularly, the researcher must demonstrate lack of variation in treatment effects among observations.\nMultilevel models are not new! From 1960 to 1978 John Tukey developed multilevel models for NBC from that were used for election forecasting.\n\n\n\n\nA statistical model is an amazing association engine\n\nFlawed causal models and confounding variables can often lead to better prediction, due to association’s predictive power. This often means that scientists can be systematically misled by predictive accuracy.\nThus, being mindful of the underlying causal assumptions is a mandatory component of statistical modeling. This is most often achieved through construction of a causal model, from which multiple statistical models can be developed.\nGraphical causal models, such as directed acyclic graphs (DAGs), are helpful tools allowing for the visualization of causal relationships. DAGs are developed outside of the data, usually in deep consultation with domain experts.\nToday’s dominating method of causal inference is the causal salad, wherein control variables are carefully tossed into the models, toward the aim of developing a causal narrative. When designing a model to be later used for interventions, it is important that the causal relationships, especially the controlling effects, are explainable.",
    "crumbs": [
      "Book Notes",
      "Chapter 1: The Golem of Prague"
    ]
  },
  {
    "objectID": "book-notes/chapter-01.html#summary",
    "href": "book-notes/chapter-01.html#summary",
    "title": "Chapter 1: The Golem of Prague",
    "section": "",
    "text": "McElreath makes the argument that instead of working with a plethora of black-box models, we should aim to develop the skills to analyze null hypotheses. The tools introduced – Bayesian data analysis, model comparison, multilevel modeling, and graphical causal models – serve this aim.\n\n\n\nChapters 2-3: Intro to Bayesian inference\nChapters 4-8: Multiple linear regression\nChapters 9-12: Generalized linear models\nChapters 13-16: Multilevel models\nChapter 17: Address issues from 1st edition\n\nEach chapter ends with exercises across difficulty levels. The more difficult problems introduce new material and challenges.",
    "crumbs": [
      "Book Notes",
      "Chapter 1: The Golem of Prague"
    ]
  },
  {
    "objectID": "course-notes/lecture-02.html",
    "href": "course-notes/lecture-02.html",
    "title": "Lecture 2: The Garden of Forking Data",
    "section": "",
    "text": "The globe tossing example from chapter two presents an interesting example of causal influence.\n\n\nShow code\nlibrary(ggdag)\n\ndag &lt;-\n  dagify(\n    W ~ N + p,\n    L ~ N + p,\n    coords = time_ordered_coords()\n  )\n\nggdag(dag) + theme_dag()\n\n\n\n\n\n\n\n\n\nHere, N represents the number of times a globe is tossed, with W being the number of times the globe is caught with your finger on water, and L being the catches on land. p represents the proportion of the earth’s surface covered by water.\nThe DAG says the “the proportion of the earth covered by water and the number of tosses influence the both number of water catches and the number of land catches,” an obvious and profound result.\n\n\n\n\nFor each possible explanation of the sample, Count all the ways the sample could happen. Explanations with more ways to produce the sample are more plausible.\n\nStatistical Rethinking 2023 - 02 - The Garden of Forking Data (9:22)\n\n\n\nBayesian models learn and adjust their estimates when reading new data:\n\n\nShow code\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.stats as st\n\nx = np.linspace(0, 1)\nfig, axes = plt.subplots(3, 3, figsize=(10, 10))\naxes = axes.flatten()\n\n# 1 = Water, 0 = Land\ntosses = [1, 0, 0, 1, 0, 1, 1, 1, 1]\nlabel = \"\"\n\n# Calculate the maximum y-value across all distributions\nmax_y = 0\nfor i in range(1, len(tosses) + 1):\n    W = sum(tosses[0:i])\n    L = i - W\n    y_vals = st.beta.pdf(x, W + 1, L + 1)\n    max_y = max(max_y, np.max(y_vals))\n\n# Add a small margin to the max_y for plotting\nmax_y *= 1.1\n\n# Reset label for plotting\nlabel = \"\"\n\n# each iteration represents a learning iteration\nfor i in range(1, len(tosses) + 1):\n    if tosses[i - 1] == 0:\n        label += \"L\"\n    else:\n        label += \"W\"\n    W = sum(tosses[0:i])\n    L = i - W\n    axes[i - 1].axvline(x=0.71, color=\"red\", linestyle=\"dashed\");\n    axes[i - 1].plot(x, st.beta.pdf(x, W + 1, L + 1), \"b-\");\n    axes[i - 1].set_title(label);\n    axes[i - 1].set_xlabel(\"p\");\n    axes[i - 1].set_ylabel(\"Density\");\n    axes[i - 1].set_ylim(0, max_y);\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nInstead of the model “learning” the posterior, you can sample from the posterior directly:\n\n\nShow code\nfrom IPython.display import HTML\nimport matplotlib.animation as animation\n\nfig, ax = plt.subplots(1, 3, figsize=(10.5, 3.5))\npost_preds = []\n\n\ndef hmcmc(frame, W=6, L=3):\n    # Clear axes before replotting\n    for ax_ in ax:\n        ax_.clear()\n\n    # Sample from Posterior\n    sample = st.beta.rvs(W + 1, L + 1)\n    post_preds.append(sample * (W + L))\n\n    # Plot 1: The Beta PDF + Current Sample\n    ax[0].plot(x, st.beta.pdf(x, W + 1, L + 1), \"b-\")\n    ax[0].axvline(x=sample, color=\"red\", linestyle=\"dashed\", alpha=0.6)\n    ax[0].set_title(f\"Analytical PDF (Sample: {sample:.2f})\")\n\n    # Plot 2: Predictive Distribution (Binomial)\n    pred_samples = st.binom.rvs(n=W + L, p=sample, size=1000)\n    ax[1].hist(\n        pred_samples,\n        bins=np.arange(W + L + 2) - 0.5,\n        color=\"skyblue\",\n        edgecolor=\"black\",\n    )\n    ax[1].set_title(\"Predictive Distribution\")\n    ax[1].set_ylim(0, 350)\n\n    # Plot 3: Posterior Predictive\n    ax[2].hist(\n        post_preds,\n        bins=np.arange(W + L + 2) - 0.5,\n        color=\"orange\",\n        edgecolor=\"black\",\n    )\n    ax[2].set_title(f\"Posterior Predictive (N={len(post_preds)})\")\n    ax[2].set_ylim(0, 45)\n\n\nani = animation.FuncAnimation(fig=fig, func=hmcmc, frames=100, interval=50)\nplt.close()\nHTML(ani.to_jshtml())\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect",
    "crumbs": [
      "Course Notes",
      "Lecture 2: The Garden of Forking Data"
    ]
  },
  {
    "objectID": "course-notes/lecture-02.html#globe-tossing-dag",
    "href": "course-notes/lecture-02.html#globe-tossing-dag",
    "title": "Lecture 2: The Garden of Forking Data",
    "section": "",
    "text": "The globe tossing example from chapter two presents an interesting example of causal influence.\n\n\nShow code\nlibrary(ggdag)\n\ndag &lt;-\n  dagify(\n    W ~ N + p,\n    L ~ N + p,\n    coords = time_ordered_coords()\n  )\n\nggdag(dag) + theme_dag()\n\n\n\n\n\n\n\n\n\nHere, N represents the number of times a globe is tossed, with W being the number of times the globe is caught with your finger on water, and L being the catches on land. p represents the proportion of the earth’s surface covered by water.\nThe DAG says the “the proportion of the earth covered by water and the number of tosses influence the both number of water catches and the number of land catches,” an obvious and profound result.",
    "crumbs": [
      "Course Notes",
      "Lecture 2: The Garden of Forking Data"
    ]
  },
  {
    "objectID": "course-notes/lecture-02.html#bayesian-data-analysis",
    "href": "course-notes/lecture-02.html#bayesian-data-analysis",
    "title": "Lecture 2: The Garden of Forking Data",
    "section": "",
    "text": "For each possible explanation of the sample, Count all the ways the sample could happen. Explanations with more ways to produce the sample are more plausible.\n\nStatistical Rethinking 2023 - 02 - The Garden of Forking Data (9:22)",
    "crumbs": [
      "Course Notes",
      "Lecture 2: The Garden of Forking Data"
    ]
  },
  {
    "objectID": "course-notes/lecture-02.html#bayesian-learning",
    "href": "course-notes/lecture-02.html#bayesian-learning",
    "title": "Lecture 2: The Garden of Forking Data",
    "section": "",
    "text": "Bayesian models learn and adjust their estimates when reading new data:\n\n\nShow code\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.stats as st\n\nx = np.linspace(0, 1)\nfig, axes = plt.subplots(3, 3, figsize=(10, 10))\naxes = axes.flatten()\n\n# 1 = Water, 0 = Land\ntosses = [1, 0, 0, 1, 0, 1, 1, 1, 1]\nlabel = \"\"\n\n# Calculate the maximum y-value across all distributions\nmax_y = 0\nfor i in range(1, len(tosses) + 1):\n    W = sum(tosses[0:i])\n    L = i - W\n    y_vals = st.beta.pdf(x, W + 1, L + 1)\n    max_y = max(max_y, np.max(y_vals))\n\n# Add a small margin to the max_y for plotting\nmax_y *= 1.1\n\n# Reset label for plotting\nlabel = \"\"\n\n# each iteration represents a learning iteration\nfor i in range(1, len(tosses) + 1):\n    if tosses[i - 1] == 0:\n        label += \"L\"\n    else:\n        label += \"W\"\n    W = sum(tosses[0:i])\n    L = i - W\n    axes[i - 1].axvline(x=0.71, color=\"red\", linestyle=\"dashed\");\n    axes[i - 1].plot(x, st.beta.pdf(x, W + 1, L + 1), \"b-\");\n    axes[i - 1].set_title(label);\n    axes[i - 1].set_xlabel(\"p\");\n    axes[i - 1].set_ylabel(\"Density\");\n    axes[i - 1].set_ylim(0, max_y);\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Course Notes",
      "Lecture 2: The Garden of Forking Data"
    ]
  },
  {
    "objectID": "course-notes/lecture-02.html#sampling-from-the-posterior",
    "href": "course-notes/lecture-02.html#sampling-from-the-posterior",
    "title": "Lecture 2: The Garden of Forking Data",
    "section": "",
    "text": "Instead of the model “learning” the posterior, you can sample from the posterior directly:\n\n\nShow code\nfrom IPython.display import HTML\nimport matplotlib.animation as animation\n\nfig, ax = plt.subplots(1, 3, figsize=(10.5, 3.5))\npost_preds = []\n\n\ndef hmcmc(frame, W=6, L=3):\n    # Clear axes before replotting\n    for ax_ in ax:\n        ax_.clear()\n\n    # Sample from Posterior\n    sample = st.beta.rvs(W + 1, L + 1)\n    post_preds.append(sample * (W + L))\n\n    # Plot 1: The Beta PDF + Current Sample\n    ax[0].plot(x, st.beta.pdf(x, W + 1, L + 1), \"b-\")\n    ax[0].axvline(x=sample, color=\"red\", linestyle=\"dashed\", alpha=0.6)\n    ax[0].set_title(f\"Analytical PDF (Sample: {sample:.2f})\")\n\n    # Plot 2: Predictive Distribution (Binomial)\n    pred_samples = st.binom.rvs(n=W + L, p=sample, size=1000)\n    ax[1].hist(\n        pred_samples,\n        bins=np.arange(W + L + 2) - 0.5,\n        color=\"skyblue\",\n        edgecolor=\"black\",\n    )\n    ax[1].set_title(\"Predictive Distribution\")\n    ax[1].set_ylim(0, 350)\n\n    # Plot 3: Posterior Predictive\n    ax[2].hist(\n        post_preds,\n        bins=np.arange(W + L + 2) - 0.5,\n        color=\"orange\",\n        edgecolor=\"black\",\n    )\n    ax[2].set_title(f\"Posterior Predictive (N={len(post_preds)})\")\n    ax[2].set_ylim(0, 45)\n\n\nani = animation.FuncAnimation(fig=fig, func=hmcmc, frames=100, interval=50)\nplt.close()\nHTML(ani.to_jshtml())\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect",
    "crumbs": [
      "Course Notes",
      "Lecture 2: The Garden of Forking Data"
    ]
  }
]